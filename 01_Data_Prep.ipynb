{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00_Data_Prep\n",
    "Prepare Orange Juice sales data to be passed into Azure's Many Models solution.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.opendatasets import OjSalesSimulated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Download data locally\n",
    "Establish how many store-brand timeseries we wish to download and save them locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dataset size\n",
    "dataset_maxfiles = 10 # Set to 11973 or 0 to get all the files\n",
    "\n",
    "# Pull all of the data\n",
    "oj_sales_files = OjSalesSimulated.get_file_dataset()\n",
    "\n",
    "# Pull only the first `dataset_maxfiles` files\n",
    "if dataset_maxfiles:\n",
    "    oj_sales_files = oj_sales_files.take(dataset_maxfiles)\n",
    "\n",
    "# Create a folder to download\n",
    "target_path = 'oj_sales_data' \n",
    "os.makedirs(target_path, exist_ok=True)\n",
    "\n",
    "# Download the data\n",
    "oj_sales_files.download(target_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Data Prep\n",
    "To create a realistic example, the \"Revenue\" and \"Price\" columns are removed, as leaving them in would introduce data leakage. The \"Advert\" column is also removed to create a purely autoregressive example. In a real solution, other available features that were found to be statistically significant and had known future values could be added to create a more accurate forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders\n",
    "train_data_path = os.path.join(target_path, \"upload_data\")\n",
    "os.makedirs(train_data_path, exist_ok=True)\n",
    "\n",
    "# Get list of files\n",
    "files_list = [os.path.join(path, f) for path, _, files in os.walk(target_path) for f in files\n",
    "          if path not in (train_data_path)]\n",
    "\n",
    "# Create dataframes\n",
    "data = pd.DataFrame()\n",
    "\n",
    "for file in files_list:\n",
    "    if 'Store' in file:\n",
    "        file_name = os.path.basename(file)\n",
    "        file_extension = os.path.splitext(file_name)[1].lower()\n",
    "        df = pd.read_csv(file)\n",
    "        df = df.drop(columns=['Revenue', 'Price'])\n",
    "\n",
    "        data = pd.concat([data, df])\n",
    "\n",
    "data.to_csv(os.path.join(train_data_path, 'oj_sales.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Upload to Blob\n",
    "Upload the dataframe to Blob storage where Azure ML can then reference it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to worksapce\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Connect to default datastore\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# Remove all checkpoints\n",
    "for root, dirs, files in os.walk(target_path, topdown=False):\n",
    "    for dir_name in dirs:\n",
    "        if dir_name == \".ipynb_checkpoints\":\n",
    "            folder_path = os.path.join(root, dir_name)\n",
    "            shutil.rmtree(folder_path)\n",
    "            print(f\"Folder '{folder_path}' removed.\")\n",
    "\n",
    "# Upload data\n",
    "ds_train_path = target_path + '/data'\n",
    "datastore.upload(src_dir=train_data_path, target_path=ds_train_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Register as dataset in AzureML\n",
    "Register the uploaded data as a dataset in AzureML to make it easily accessable from Azure ML services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file datasets\n",
    "ds = Dataset.Tabular.from_delimited_files([(datastore, os.path.join(ds_train_path, 'oj_sales.csv'))])\n",
    "\n",
    "# Register the file datasets\n",
    "dataset_name = 'oj_data_small' if 0 < dataset_maxfiles < 11973 else 'oj_data'\n",
    "dataset_name = dataset_name + '_train'\n",
    "ds.register(ws, dataset_name, create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that the data is available in AzureML, it's time to create the training and inference pipelines. Follow the steps in [02_Pipeline.ipynb](0e_Pipeline.ipynb) for that."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_Pipeline\n",
    "Train many forecasting models per product\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import azureml.core\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.train.automl.automlconfig import AutoMLConfig\n",
    "from azureml.pipeline.core import Schedule, ScheduleRecurrence\n",
    "from azureml.core import Workspace, Datastore, Experiment, Environment\n",
    "from azureml.contrib.automl.pipeline.steps import AutoMLPipelineBuilder\n",
    "from azureml.core.compute import ComputeTarget, ComputeInstance, AmlCompute\n",
    "from azureml.automl.core.forecasting_parameters import ForecastingParameters\n",
    "from azureml.train.automl.runtime._many_models.many_models_parameters import ManyModelsTrainParameters\n",
    "from azureml.train.automl.runtime._many_models.many_models_parameters import ManyModelsInferenceParameters\n",
    "\n",
    "# Connect to  workspace\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create control file to store all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_file = {\n",
    "    'train_experiment': 'oj_training_pipeline',\n",
    "    'inference_experiment': 'oj_inference_pipeline',\n",
    "    'compute': {\n",
    "        'name': '_',\n",
    "        'version'\n",
    "        'source_location' : './sripts',\n",
    "        'datastore_name' : '_',\n",
    "    },\n",
    "    'environment' : {\n",
    "        'name': 'oj_env',\n",
    "        'version': 1\n",
    "    },\n",
    "    'data': {\n",
    "        'data_asset': 'oj_data_small_train',\n",
    "        'output': 'oj_sales_data',\n",
    "        'forecast': 'oj_forecast'\n",
    "    },\n",
    "    'pipeline': {\n",
    "        'source': './scripts',\n",
    "    },\n",
    "    'forecasting': {\n",
    "        'horizon': 8,\n",
    "        'partitions': ['Store', 'Brand'],\n",
    "        'lags': [1,2,4,8],\n",
    "        'time_column_name': 'WeekStarting',\n",
    "        'cv_step_size': 'auto',\n",
    "        'target_rolling_window_size': 4,\n",
    "        'seasonality': 'auto',\n",
    "        'use_stl': 'season_trend'\n",
    "    },\n",
    "    'automl': {\n",
    "        'iteration_timeout_minutes': 60,\n",
    "        'iterations': 10,\n",
    "        'experiment_timeout_hours': 3,\n",
    "        'label_column_name': 'Quantity',\n",
    "        'n_cross_validations': 'auto',\n",
    "        'track_child_runs': False,\n",
    "        'allowed_models': ['Prophet'],\n",
    "    },\n",
    "    'training': {\n",
    "        'node_count': 5,\n",
    "        'process_count_per_node': 2,\n",
    "        'run_invocation_timeout': 50000\n",
    "    },\n",
    "    'inference': {\n",
    "        'node_count': 5,\n",
    "        'process_count_per_node': 2,\n",
    "        'run_invocation_timeout': 50000\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0 Create Env (run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Conda environment\n",
    "env = Environment.from_conda_specification(name=control_file['environment']['name'], \n",
    "                                           file_path='environment.yml')\n",
    "\n",
    "# Register environment\n",
    "env.register(workspace=ws)\n",
    "\n",
    "# Build environment\n",
    "env.build(workspace=ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Connect to Datastore and Compute, and Name the Training Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check SDK verison\n",
    "print(\"Azure ML SDK Version:\", azureml.core.VERSION)\n",
    "\n",
    "# Connect to datastores\n",
    "dstore = ws.get_default_datastore()\n",
    "\n",
    "# Define the compute instance name\n",
    "compute_name = control_file['compute']['name']\n",
    "\n",
    "# Determine if the compute target exists\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute = ws.compute_targets[compute_name]\n",
    "    \n",
    "    # Check if the compute target is of the right type (ComputeInstance or AmlCompute).\n",
    "    if compute and (type(compute) is ComputeInstance or type(compute) is AmlCompute):\n",
    "        print(f'Compute instance {compute_name} is found')\n",
    "else:\n",
    "    # Raise an exception and print an error message if the compute instance is not found.\n",
    "    print(f'Compute instance {compute_name} is not found, the code cannot be executed')\n",
    "    raise Exception(\"Compute instance not found\")\n",
    "\n",
    "# Name experiment\n",
    "experiment = Experiment(ws, control_file['train_experiment'])\n",
    "print('Experiment name: ' + experiment.name)\n",
    "\n",
    "# Create a run configuration and assign compute\n",
    "run_configuration = RunConfiguration()\n",
    "run_configuration.target = compute\n",
    "\n",
    "# Assign environment\n",
    "run_configuration.environment = Environment.get(workspace = ws, \n",
    "                                                name      = control_file['environment']['name'], \n",
    "                                                version   = control_file['environment']['version'])\n",
    "\n",
    "# Print details\n",
    "output = {}\n",
    "output['SDK version']     = azureml.core.VERSION\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace']       = ws.name\n",
    "output['Resource Group']  = ws.resource_group\n",
    "output['Location']        = ws.location\n",
    "output['Datastore name']  = dstore.name\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call dataset\n",
    "sales_data = Dataset.get_by_name(ws, name = control_file['data']['data_asset'])\n",
    "# If there was an external features dataset, it would be called here\n",
    "#features = Dataset.get_by_name(ws, name = 'features')\n",
    "\n",
    "# Create an instance of the data_storage class for managing storage locations.\n",
    "output_path = OutputFileDatasetConfig(destination = (dstore, f\"{control_file['data']['output']}/train\")).as_upload(overwrite=True).read_parquet_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Build Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast lags list as string\n",
    "lags = ', '.join([str(item) for item in control_file['forecasting']['lags']])\n",
    "partitions = ', '.join([str(item) for item in control_file['forecasting']['partitions']])\n",
    "\n",
    "# Python data prep step\n",
    "data_prep_step = PythonScriptStep(\n",
    "    script_name      = \"preprocessing.py\",\n",
    "    source_directory = control_file['pipeline']['source'],\n",
    "    inputs           = [sales_data.as_named_input('train_data')], #features would also be passed in here\n",
    "    arguments        = [\"--step\", \"train\",\n",
    "                        \"--output_path\", output_path, \n",
    "                        \"--lags\", lags,\n",
    "                        \"--horizon\", control_file['forecasting']['horizon'],\n",
    "                        \"--time_column_name\", control_file['forecasting']['time_column_name'],\n",
    "                        \"--label_column_name\", control_file['automl']['label_column_name'],\n",
    "                        \"--partitions\", partitions],\n",
    "    compute_target   = compute,\n",
    "    runconfig        = run_configuration,\n",
    "    allow_reuse      = False\n",
    ")\n",
    "\n",
    "# Forecasting Parameters\n",
    "forecasting_parameters = ForecastingParameters(\n",
    "    time_column_name            = control_file['forecasting']['time_column_name'],\n",
    "    forecast_horizon            = control_file['forecasting']['horizon'],\n",
    "    time_series_id_column_names = control_file['forecasting']['partitions'],\n",
    "    cv_step_size                = control_file['forecasting']['cv_step_size'],\n",
    "    target_lags                 = control_file['forecasting']['lags'],\n",
    "    target_rolling_window_size  = control_file['forecasting']['target_rolling_window_size'],\n",
    "    seasonality                 = control_file['forecasting']['seasonality'],\n",
    "    use_stl                     = control_file['forecasting']['use_stl']\n",
    ")\n",
    "\n",
    "# AutoML Config\n",
    "automl_settings = AutoMLConfig(\n",
    "    task                      = \"forecasting\",\n",
    "    primary_metric            = \"normalized_root_mean_squared_error\",\n",
    "    iteration_timeout_minutes = control_file['automl']['iteration_timeout_minutes'],\n",
    "    iterations                = control_file['automl']['iterations'],\n",
    "    experiment_timeout_hours  = control_file['automl']['experiment_timeout_hours'],\n",
    "    label_column_name         = control_file['automl']['label_column_name'],\n",
    "    n_cross_validations       = control_file['automl']['n_cross_validations'],\n",
    "    track_child_runs          = control_file['automl']['track_child_runs'],\n",
    "    allowed_models            = control_file['automl']['allowed_models'],\n",
    "    forecasting_parameters    = forecasting_parameters    \n",
    ")\n",
    "\n",
    "# Many Models Training Parameters\n",
    "mm_paramters = ManyModelsTrainParameters(\n",
    "    automl_settings        = automl_settings, \n",
    "    partition_column_names = control_file['forecasting']['partitions']\n",
    ")\n",
    "\n",
    "# Define train step\n",
    "train_step = AutoMLPipelineBuilder.get_many_models_train_steps(\n",
    "    experiment                = experiment,\n",
    "    train_data                = output_path.as_input('train_10_models'),\n",
    "    compute_target            = compute,\n",
    "    node_count                = control_file['training']['node_count'],\n",
    "    process_count_per_node    = control_file['training']['process_count_per_node'],\n",
    "    run_invocation_timeout    = control_file['training']['run_invocation_timeout'],\n",
    "    train_pipeline_parameters = mm_paramters,\n",
    ")\n",
    "\n",
    "steps = [data_prep_step, train_step]\n",
    "print('Pipeline built')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Run the Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and submit pipeline\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "training_run = experiment.submit(pipeline)\n",
    "\n",
    "print(f'Experiment Name: {training_run.experiment.name}, Run ID: {training_run.id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0 Publish the Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish(name = 'oj_train_many_models',\n",
    "                                     description = 'Train many models on oj sales data',\n",
    "                                     version = '1',\n",
    "                                     continue_on_step_failure = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.0 Schedule the Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline_id = published_pipeline.id\n",
    "\n",
    "recurrence = ScheduleRecurrence(frequency=\"Month\", interval=1, start_time=\"2020-01-01T09:00:00\")\n",
    "recurring_schedule = Schedule.create(ws, name=\"automl_training_recurring_schedule\", \n",
    "                            description=\"Schedule Training Pipeline to run on the first day of every month\",\n",
    "                            pipeline_id=training_pipeline_id, \n",
    "                            experiment_name=experiment.name, \n",
    "                            recurrence=recurrence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.0 Name the Inference Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name experiment\n",
    "experiment = Experiment(ws, control_file['inference_experiment'])\n",
    "print('Experiment name: ' + experiment.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.0 Load Inference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call dataset\n",
    "sales_data = Dataset.get_by_name(ws, name = control_file['data']['data_asset'])\n",
    "\n",
    "# Create an instance of the data_storage class for managing storage locations.\n",
    "output_path = OutputFileDatasetConfig(destination = (dstore, f\"{control_file['data']['output']}/inference\")).as_upload(overwrite=True).read_parquet_files()\n",
    "\n",
    "# Define the forecast output location\n",
    "output_file_name = \"forecast.csv\"\n",
    "forecast_path = OutputFileDatasetConfig(destination = (dstore, f\"{control_file['data']['data_output']}/forecast\")).register_on_complete(name=control_file['data']['forecast'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.0 Build Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast lags list as string\n",
    "lags = ', '.join([str(item) for item in control_file['forecasting']['lags']])\n",
    "\n",
    "# Python data prep step\n",
    "data_prep_step = PythonScriptStep(\n",
    "    script_name      = \"preprocessing.py\",\n",
    "    source_directory = control_file['pipeline']['source'],\n",
    "    inputs           = [sales_data.as_named_input('train_data')],\n",
    "    arguments        = [\"--step\", \"inference\",\n",
    "                        \"--output_path\", output_path, \n",
    "                        \"--lags\", lags,\n",
    "                        \"--horizon\", control_file['forecasting']['horizon'], \n",
    "                        \"--label_column_name\", control_file['automl']['label_column_name']],\n",
    "    compute_target   = compute,\n",
    "    runconfig        = run_configuration,\n",
    "    allow_reuse      = False\n",
    ")\n",
    "\n",
    "# Many Models Inference Parameters\n",
    "mm_parameters = ManyModelsInferenceParameters(\n",
    "    partition_column_names = control_file['forecasting']['partitions'],\n",
    "    time_column_name       = control_file['forecasting']['time_column_name'],\n",
    "    target_column_name     = control_file['forecasting']['label_column_name'],\n",
    "    inference_type         = \"forecast\",\n",
    "    forecast_mode          = \"recursive\",\n",
    "    forecast_quantiles     = [0.1, 0.5, 0.9]\n",
    ")\n",
    "\n",
    "# Create inference step\n",
    "inference_step = AutoMLPipelineBuilder.get_many_models_batch_inference_steps(\n",
    "    experiment                    = experiment, \n",
    "    inference_data                = output_path,\n",
    "    compute_target                = compute,\n",
    "    node_count                    = control_file['inference']['node_count'],\n",
    "    process_count_per_node        = control_file['inference']['process_count_per_node'],\n",
    "    run_invocation_timeout        = control_file['inference']['run_invocation_timeout'],\n",
    "    output_datastore              = forecast_path,\n",
    "    train_experiment_name         = training_run.experiment.name,\n",
    "    train_run_id                  = training_run.id,\n",
    "    inference_pipeline_parameters = mm_parameters,\n",
    "    append_row_file_name          = output_file_name\n",
    ")\n",
    "\n",
    "steps = [data_prep_step, inference_step]\n",
    "print('Pipeline built')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.0 Run the Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and submit pipeline\n",
    "pipeline = Pipeline(workspace = ws, steps = steps)\n",
    "inference_run = experiment.submit(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.0 Publish Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish(name = 'oj_inference_many_models',\n",
    "                                     description = 'Forecast many models on oj sales data',\n",
    "                                     version = '1',\n",
    "                                     continue_on_step_failure = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.0 Schedule Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline_id = published_pipeline.id\n",
    "\n",
    "recurrence = ScheduleRecurrence(frequency=\"Month\", interval=1, start_time=\"2020-01-01T09:00:00\")\n",
    "recurring_schedule = Schedule.create(ws, name=\"automl_training_recurring_schedule\", \n",
    "                            description=\"Schedule Training Pipeline to run on the first day of every month\",\n",
    "                            pipeline_id=inference_pipeline_id, \n",
    "                            experiment_name=experiment.name, \n",
    "                            recurrence=recurrence)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
